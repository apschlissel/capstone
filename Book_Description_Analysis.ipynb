{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Book Description Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apschlissel/capstone/blob/main/Book_Description_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of the Description Field of the Data**"
      ],
      "metadata": {
        "id": "TkqQPaIk-gmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the Data"
      ],
      "metadata": {
        "id": "6omwMK_N-zoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Import Libraries\n",
        "import string\n",
        "import pandas as pd\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import itertools\n",
        "import scipy.sparse as sp\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "-ncMz7zH-jCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61633f08-d0fc-4426-ef76-766eeb4b8617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVaO4PlYxdad",
        "outputId": "ef2c4af2-6c46-40ee-d22b-d96bf2ef9d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Set Tools\n",
        "stemmer = SnowballStemmer(\"english\") \n",
        "wn = nltk.WordNetLemmatizer()\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords.append(\"none\")\n",
        "stopwords.append(\"nan\")"
      ],
      "metadata": {
        "id": "Ptl3UB_Svx98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQyJdLBV-d56"
      },
      "outputs": [],
      "source": [
        "## Read in Data\n",
        "def file_input(filename):\n",
        "    try:\n",
        "        #making my dataframe from excel file and printing out\n",
        "        data = pd.read_excel(filename, dtype='object')\n",
        "        return data\n",
        "        \n",
        "    except Exception:\n",
        "        #making my dataframe from csv file and printing out \n",
        "        data = pd.read_csv(filename, dtype='object')\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def co_occurance_matricies(data):\n",
        "    \n",
        "    columns = data.loc[:,data.columns != \"Unique_id\"]\n",
        "    col_count_vectorizers = {}\n",
        "    matricies = {}\n",
        "    for col in columns:\n",
        "        \n",
        "        print(col)\n",
        "        corpus = itertools.chain.from_iterable(data[col])\n",
        "        vocab = set(corpus)\n",
        "        varnames = tuple(sorted(vocab))\n",
        "        document = data[col]\n",
        "        rows, cols, vals = [], [], []\n",
        "        voc2id = dict(zip(vocab, range(len(vocab))))\n",
        "        id2voc = dict(zip(range(len(vocab)), vocab))\n",
        "        for r, d in enumerate(document):\n",
        "            for e in d:\n",
        "                rows.append(r)\n",
        "                cols.append(voc2id[e])\n",
        "                vals.append(1)\n",
        "        X = sp.csr_matrix((vals, (rows, cols)))\n",
        "        Xc = (X.T * X)\n",
        "        Xc.setdiag(0)\n",
        "        Xc = Xc.todense()\n",
        "        sums = np.sum(Xc, axis=0)\n",
        "        sums = sums.tolist()\n",
        "        sums = sums[0]\n",
        "        total_count = sum(sums)\n",
        "        Xc = sp.csr_matrix(Xc)\n",
        "        min_num = min(sums)\n",
        "        min_prob = min_num/total_count\n",
        "        non_zeros = Xc.nonzero()\n",
        "        row_coords = []\n",
        "        col_coords = []\n",
        "        data_coord = []\n",
        "        for i in range(0, len(non_zeros[0])):\n",
        "            print(i/len(non_zeros[0]))\n",
        "            x = non_zeros[0][i]\n",
        "            y = non_zeros[1][i]\n",
        "            word = Xc[x, y]\n",
        "            row_sum = sums[x]/total_count\n",
        "            column_sum = sums[y]/total_count\n",
        "            uniq_word_count = word/total_count\n",
        "            #Pa = (column_sum**.75)/((column_sum**.75)+(min_prob**.75))\n",
        "            wppmi = max(math.log2((uniq_word_count/(row_sum*column_sum))), 0)\n",
        "            row_coords.append(x)\n",
        "            col_coords.append(y)\n",
        "            data_coord.append(wppmi)\n",
        "            \n",
        "        \n",
        "        row_coords = np.array(row_coords) \n",
        "        col_coords = np.array(col_coords)\n",
        "        data_coord = np.array(data_coord)\n",
        "        max_wppmi = max(data_coord) + 5\n",
        "        wppmi_matrix = sp.csr_matrix((data_coord, (row_coords, col_coords))\n",
        "                                  ,shape = (max(non_zeros[0]+1), max(non_zeros[1]+1)))\n",
        "        \n",
        "        wppmi_matrix = wppmi_matrix.toarray()\n",
        "        similarity_enc = {}\n",
        "        i = 0\n",
        "        for voc_word in voc2id:\n",
        "            print(i/len(voc2id))\n",
        "            i += 1\n",
        "            mat_row = wppmi_matrix[voc2id[voc_word]]\n",
        "            sorted_mat = np.sort(mat_row)\n",
        "            sorted_mat = sorted_mat[::-1]\n",
        "            top5 = sorted_mat[:4]\n",
        "            top5_index = {}\n",
        "            mat_row = mat_row.tolist()\n",
        "            \n",
        "            for num in top5:\n",
        "                index = mat_row.index(num)\n",
        "                mat_row.pop(index)\n",
        "                index_word = id2voc[index]\n",
        "                top5_index[index_word] = (index, num)\n",
        "                \n",
        "            similarity_enc[voc_word] = top5_index\n",
        "            \n",
        "    return similarity_enc"
      ],
      "metadata": {
        "id": "hgU3mLxh_RnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_unique_mentions(dat):\n",
        "    \n",
        "    \n",
        "\n",
        "    total_count = {}\n",
        "    \n",
        "    for col in dat:\n",
        "        col_count = {}\n",
        "        col_counts = {}\n",
        "        docs = dat[col]\n",
        "        count = 0\n",
        "        for doc in docs:\n",
        "            sentence = []\n",
        "            for char in doc:\n",
        "                if char in sentence:\n",
        "                    continue\n",
        "                else:\n",
        "                    if char in total_count:\n",
        "                        total_count[char] = total_count[char] + 1\n",
        "                    else:\n",
        "                        total_count[char] = 1\n",
        "                    if char in col_count:\n",
        "                        col_count[char] = col_count[char] + 1\n",
        "                    else:\n",
        "                        col_count[char] = 1\n",
        "        \n",
        "        col_counts[col] = col_count\n",
        "                    \n",
        "\n",
        "        \n",
        "    return [total_count, col_count]"
      ],
      "metadata": {
        "id": "Q7YlXANk_Rsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orig_df = file_input(\"./books_filtered_colsreduced.csv\")\n",
        "df = orig_df[['description']].copy()\n",
        "data = df\n",
        "data = data.astype(str)\n",
        "df = df.astype(str)\n",
        "index = 0\n",
        "vocab = []\n",
        "for col in df:\n",
        "    index += 1\n",
        "    df[col] = df[col].str.lower()\n",
        "    df[col] = df[col].apply(word_tokenize)\n",
        "    data[col] = data[col].apply(word_tokenize)\n",
        "    df[col] = df[col].apply(lambda words: [word for word in words if word not in stopwords])\n",
        "    df[col] = df[col].apply(lambda x: [stemmer.stem(y) for y in x])\n",
        "    corpus = list(itertools.chain.from_iterable(df[col]))\n",
        "    vocab = vocab + corpus\n",
        "    print(str(index) + 'out of' + str(df.shape[1]) + 'columns processed')\n",
        "      \n",
        "vocab = set(vocab)\n",
        "index = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8a3A3DrwXhV",
        "outputId": "6a05f5dd-bba6-490e-ca7c-f981a37339e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1out of1columns processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_similarities = co_occurance_matricies(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Jb5mKfEwXmD",
        "outputId": "d820d1d7-a32f-4af0-ab6b-b582dd5309af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "description\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tot_count, col_count = count_unique_mentions(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "sCy789scwXqS",
        "outputId": "a94552a1-3c03-4d86-da4c-c23502739c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5b80734ce581>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtot_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_unique_mentions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'count_unique_mentions' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dJkEIKAVwXuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pHjULVYsrH6T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}